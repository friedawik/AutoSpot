from glob import glob
from typing import Optional, Union, Tuple

import os
from IPython import embed
import numpy as np
import cv2

import imageio.v3 as imageio
from matplotlib import pyplot as plt
from skimage.measure import label as connected_components

import torch
from torch_em.util.util import get_random_colors

from micro_sam.util import get_sam_model
from micro_sam.evaluation import inference
from micro_sam.evaluation.model_comparison import _enhance_image
from micro_sam.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data
from micro_sam.evaluation.evaluation import run_evaluation, run_evaluation_for_iterative_prompting
from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation
from segmentation_tools import pixel_level_metrics

"""
This code runs a finetuned SAM2 prediction with the default prompt grid generated by SAM2.
Note that amg must be set to amg=True.

The following post-processing threshold were tested. Modifying them will change 
the segmentation outcome:

        stability_score_thresh=0.65,
        min_mask_region_area=2,
        box_nms_thresh=0.45,
        pred_iou_thresh=0.75,

"""


def run_automatic_mask_generation(
    image: np.ndarray,
    ndim: int,
    checkpoint_path: Optional[Union[os.PathLike, str]] = None,
    model_type: str = "vit_b",
    device: Optional[Union[str, torch.device]] = None,
    tile_shape: Optional[Tuple[int, int]] = None,
    halo: Optional[Tuple[int, int]] = None,
):
    """Automatic Mask Generation (AMG) is the automatic segmentation method offered by SAM.

    NOTE: AMG is supported for both Segment Anything models and `µsam` models.

    Args:
        image: The input image.
        ndim: The number of dimensions for the input data.
        checkpoint_path: The path to stored checkpoints.
        model_type: The choice of the SAM / `µsam` model.
        device: The device to run the model inference.
        tile_shape: The tile shape for tiling-based segmentation.
        halo: The overlap shape on each side per tile for stitching the segmented tiles.

    Returns:
        The instance segmentation.
    """
        # Step 1: Get the 'predictor' and 'segmenter' to perform automatic instance segmentation.
    predictor, segmenter = get_predictor_and_segmenter(
        model_type=model_type,  # choice of the Segment Anything model
        checkpoint=checkpoint_path,  # overwrite to pass your own finetuned model.
        device=device,  # the device to run the model inference.
        amg=True,  # set the automatic segmentation mode to AMG.
        is_tiled=(tile_shape is not None),  # whether to run automatic segmentation with tiling.
    )

    # Step 2: Get the instance segmentation for the given image.
    prediction = automatic_instance_segmentation(
        predictor=predictor,  # the predictor for the Segment Anything model.
        segmenter=segmenter,  # the segmenter class responsible for generating predictions.
        input_path=image,  # the filepath to image or the input array for automatic segmentation.
        ndim=ndim,  # the number of input dimensions.
        tile_shape=tile_shape,  # the tile shape for tiling-based prediction.
        halo=halo,  # the overlap shape for tiling-based prediction.
        stability_score_thresh=0.65,
        min_mask_region_area=2,
        box_nms_thresh=0.45,
        pred_iou_thresh=0.75,
    )

    return prediction

device = "cuda" if torch.cuda.is_available() else "cpu" # the device/GPU used for training



root_dir = '/global/D1/homes/frieda/tools/micro-sam'

val_img_folder = '../../../data/patch_256_sam/val/images'
val_mask_folder = '../../../data/patch_256_sam/val/masks'

test_img_folder = '../../../data/patch_256_sam/test/images'
test_mask_folder = '../../../data/patch_256_sam/test/masks'


# test_img_paths = sorted([os.path.join(test_img_folder, f) for f in os.listdir(test_img_folder)])
test_mask_paths = sorted([os.path.join(test_mask_folder, f) for f in os.listdir(test_mask_folder)])
val_image_paths = sorted([os.path.join(val_img_folder, f) for f in os.listdir(val_img_folder)])
val_mask_paths = sorted([os.path.join(val_mask_folder, f) for f in os.listdir(val_mask_folder)])
test_image_paths = sorted([os.path.join(test_img_folder, f) for f in os.listdir(test_img_folder)])

# Set up model
checkpoint = os.path.join("../models", "checkpoints", "semantic_binary", "best.pt")
model_type = "vit_b"  # overwrite with your desired choice of model

# make folder for plotting results
result_folder = os.path.join(root_dir, 'results', "amg_finetune_4")
os.makedirs(result_folder, exist_ok=True)
 

# initialise score lists
iou_list = []
precision_list = []
recall_list = []
f1_list = []
acc_list = []

# Loop through all image patches to predict masks
for gt_path in test_mask_paths:
    gt_mask = imageio.imread(gt_path)
    gt_id = os.path.split(gt_path)[-1][:-10]
    print(gt_id)
    image_path = os.path.join(test_img_folder, gt_id+ '.png') # for sam data
    image = imageio.imread(image_path)
    image_plot = image.copy()

    # Get prediction
    prediction = run_automatic_mask_generation(image, ndim=2, model_type=model_type, checkpoint_path=checkpoint)

    # Set all elements greater than 0 to 1 to merge into binary mask
    prediction[prediction > 0] = 1
    prediction=prediction.astype('uint8')
    gt_mask[gt_mask > 0] = 1
    
    # Find contours in the predicted mask for plotting
    contours, _ = cv2.findContours(prediction, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Draw contours on the overlay using cv2
    cv2.drawContours(image_plot, contours, -1, (0, 255, 0), 1) # Green contours with thickness of 2
    # Plot each contour using plt.plot 
    plt.imshow(image_plot, cmap='gray')
    for contour in contours:
        # Reshape the contour to 2D for plotting to use same method as other
        contour = contour.squeeze()
        if len(contour.shape) == 2:  # Avoid empty contours
            contour =  np.vstack([contour, contour[0]])
            plt.plot(contour[:, 0], contour[:, 1], c='g', linewidth=1)
    plt.axis('off')
    # plt.title('SAM2')
    plt.savefig(os.path.join(result_folder, f'{gt_id}.png'), bbox_inches='tight', pad_inches=0)
    plt.clf()

    # Get metrics and append in list 
    precision, recall, f1_score, iou, balanced_accuracy = pixel_level_metrics(prediction, gt_mask)
    precision_list.append(precision)
    recall_list.append(recall)
    f1_list.append(f1_score)
    iou_list.append(iou)
    acc_list.append(balanced_accuracy)
  
# Calculate total metrics
total_precision = np.mean(precision_list)
total_recall = np.mean(recall_list)
total_f1 = np.mean(f1_list)
total_iou = np.mean(iou_list)
total_acc = np.mean(acc_list)

print(f'Precision: {total_precision}\nRecall: {total_recall}\nf1_score: {total_f1}\nIOU:{total_iou}\nAccuracy: {total_acc}.')
 
